1)Dump the data from sql to localdirectory 


2)load the data from local directory to sql db 


3)list all the databses 


4)list all tables in a particular database 


5)evaluate the data from table 


6)load all tables from sql and put in hdfs from a database 


7)load a particular table from db and put in hadfs 


8)load and append to it 


9)

sqoop is one of the hadoop eco system tool

sqoop main purpose is importing the data from rdbms to hdfs and exporting the data from hdfs to rdbms 

sqoop  ==>sql +  hadoop 

in scoop default number of mappers are 4 
no of reducers are zero 
why n reducers in sqoop 
  -->for aggregarion operation we require sqoop and in sqoop we require only select quiries for that mappers are sufficient 

diff b/w warehouse and target-dir  in sqoop

if wea are importing all the tabe in a particular database we require warehouse-dir 
if we are importing the data from particular table we require target-dir 

##If the table is not having a pk we have to choose numberof mappers = 1 because if a table having 1000 recordse tha table not having pk 
then 1000 records have to split t0 4 parts and in each part 250 records in map1 1-250 and in map2 251-500 like that 
but if the table not haing pk first 1-250 records are loaded into the mapper1 and mapper2 can't load because table not having pk 
we can overcome this problem by using --num-mappers 1 or -m 1 
the second option is we can also use --split-by columnname 

1.In import operation when we are using query tag the query tag must and should ends with $CONDITIONS . 
2.In eval operation when we are using query tag the query tag shouldn't end with $CONDITIONS  .


when running any sqoop job unfortunately job failed in a table we have 200 records out of 200 onlhy 50 are loaded into the hdfs now again run the hob then first 50 records will not reload again it will be taken care by metastoredb derby 

options-file must and should have only 400 permissions only 

vi file 
--connect
jdbc:mysql://localhost:3306
--username 
root 
--password 
hadoop 

:wq

1)list all the databses in rbdms mysql using sqoop

sqoop list-databases --options-file file 

2)list all tables in rdbms mysql kalyandatabse 

sqoop list-tables --connect "jdbc:mysql://localhost:3306/kalyan" --username root --password hadoop 

3)eval operation 

sqoop eval --options-file file --query 'show databases' 

4)The permissions for options-file is 400 only 

5)import all tables and put it in warehouse directory 

sqoop import --options-file file --warehouse-dir /sqoop/import-all-data/kalyan.db --exclude-tables orders,orders_items 

6)import a particular table sqoop.pet and put in hdfs with number of mappers 1 and saved as textfile

sqoop import --options-file file --table pet --target-dir /sqoop/import-data/sqoop.db/pet_m --as-textfile -m 1 

7)import a particular a table sqoop.pet with split-by option and saved as sequencefile target-dir /sqoop/inport-data/sqoop.db/pet 

sqoop import --options-file --table pet --target-dir /sqoop/import-data/sqoop.db/pet_split --as-sequencefile --split-by name 
   here split happens based on name column and no of mappers are 4 

8)case:In sqoop database there is a table called student having 36 records and having pk 
  a)using boundary query load only first 10 records and save as textfile fields lines terminated by 
  b)using boundary qury loas another 10 records append to it and number of mappers 1 fields lines terminated saved as textfile 
  c)using spli-by import id import the data here no of mappers4 lines fields terminated by save as textfile 

a)sqoop import --options-file file --table student --target-dir /sqoop/boundary-query/sqoop.db/student --boundary-query 'select 1,10 from student limit 1 ' --as-textfile --fields-terminated-by '\t' --lines-terminated-by '\n' -m 1 

b)sqoop import --options-file file --table student--target-dir /sqoop/boundary-query/sqoop.db/student/ --append --boundary-query 'select 11,20 from student limit 1' --as-textfile --fields-terminated-by '\t' --lines-terminated-by '\n' -m 1

c)sqoop import --options-file file --table student --target-dir /sqoop/boundary-query/sqoop.db/student --append --split-by id --lines-terminated-by '\n' --fields-terminated-by '\t' --as-textfile 

9)query opeartion 
 when doing query operation don't use table option because the o/p coming from query only we are loading into hdfs for that we require target-dir if table is having pk then no need to use no of mapers 1 here in example use --split-by name --num-mappers 2

sqoop import --options-file file --query 'select * from student where $CONDITIONS' --target-dir /sqoop/query/sqoop.db/student --split-by name --num-mappers 2 --fields-terminated-by '\t' --lines-terminated-by '\n' --as-textfile 

10)If a table student having 36 records load only 10 records using where condition and columns name,id 
   laod another 10 records using query operation using append remove duplicates (use incremental append 

sqoop import --options-file file --table student --target-dir /sqoop/data/sqoop.db/student --where "id <=10" --columns name,id --fields-terminated-by '\t' --lines-terminated-by '\n' --as-textfile 

sqoop import --options-file file --target-dir /sqoop/data/sqoop.db/student --query 'select * from student where id<=20 and $CONDITIONS' --as-textfile --fields-terminated-by '\t' --lines-terminated-by '\n' --append --incremental append --check-column id --last-value 10 

  when importing the data from rdbms to hdfs if a table having 1000 records we loaded some of them in to hdfs and second day we have to load remaining instead of seeing hdfs we have to do? The last value will be maintaind by metastore derby there is a concept called stat it wil maintain last value 


  There is a table in kalyan database departments .create a new table departments1 and departments2 
   create table kalyan.departments1 as select * from kalyan.departments 
   create table kalyan.departments2 as kalyan.departments 

case:import table departments1 into hdfs and export to rdbms departments2 table with same target-dir 

sqoop import --options-file file --table departments1 --target-dir /sqoop/kalyan.db/departments --as-textfile --fields-terminated-by '\t' --lines-terminated-by '\t' -m 1

#To export the data 
 a)Table must and should be present 
 b)Data and Schema should match 
 c)Sqoop validates the constraints null notnulls etc 

sqoop export --options-file file --table departments2 --export-dir /sqoop/kalyan.db/departments --num-mappers 1 --input-fields-terminated-by '\t' --input-lines-terminated-by '\n' --batch 



sqoop job :
-----------
sqoop job --list
sqoop job --show myjob1 
sqoop job --delete myjob1
sqoop job --exec myjob2 

How to create sqoop job's : 
sqoop job --create myjob1 \
  --list-databases \
  --connect " " \
  --username root \
  --password hadoop 
sqoop job list 
sqoop job --show myjob1 
sqoop job --exec myjob1 

sqoop job --create myjob2 \ 
  --list-tables \
  --connect " " \
  --username root \
  --password hadoop 
in sqoop we have an option --outdir java_files --bindir bin_files 

case: using sqoop update the row in rdbms kalyan.departments1 
      insert the data 
      query the data using select 
update the record in rdbms :
-------------------------------
sqoop eval --options-file file --query 'update departments1 set department_name = "updated" where department_id =7" '

insert the record in rdbms:
----------------------------
sqoop eval --options-file file --query 'insert into departments1 values(8,"updated") ' 

Read the data from rdbms :
-----------------------------
sqoop eval --options-file file --query 'select * from kalyan.departments1' 

sqoop import --options-file file --table departments1 --target-dir /sqoop/kalyan.db/departments1_delta --lines-terminated-by '\n' --fields-terminated-by '\t' --as-textfile -m 1 --bindir bin_files --outdir java_files 

Export the data from hdfs to rdbms using update-mode :
-------------------------------------------------------

sqoop export --options-file file --table departments2 --export-dir /sqoop/kalyan.db/departments1_delta --input-lines-terminated-by '\n' --input-fields-terminated-by '\t' --batch --update-key department_id --update-mode updateonly -m 1

sqoop export --options-file file --table departments2 --export-dir /sqoop/kalyan.db/departments1_delta --input-lines-terminated-by '\n' --input-fields-terminated-by '\t'  --batch -m 1 --update-key department_id --update-mode allowinsert outdir java_files 

case: 
------
student 
------------------------
|     id   | name      |
-------------------------
         1  |    shaym  |
-------------------------
          2 | null      |
--------------------------
       null |    siva   |
-------------------------
      null  |   null   |
------------------------


I want i place of null in id columns as not a number and in place of name column which are having nulls as NA

sqoop import \
  --options-file filename 
  --table student 
  --target-dir /sqoop/kalayn.db/student 
  --delete-target-dir 
  --as-textfile 
  --null-string "NA" 
  --null-non-string "not a number"
  --lines-terminated-by '\n' 
  --fields-terminated-by '\t' 
  --num-mappers 1 


Note:if a table not having pk must and should use number of mappers are 1 
 if a table not having pk we want number of mappers are 4 then use --split-by 

How to improve the performance of a sqoop job: 
------------------------------------------------
 This can be done by incresing the number of mappers 
   --batch  
   bounch of records 
   fetch-size

--fetch-size to import multiple records in a single shot of time 

















































