1)Dump the data from sql to localdirectory 


2)load the data from local directory to sql db 


3)list all the databses 


4)list all tables in a particular database 


5)evaluate the data from table 


6)load all tables from sql and put in hdfs from a database 


7)load a particular table from db and put in hadfs 


8)load and append to it 


9)

sqoop is one of the hadoop eco system tool

sqoop main purpose is importing the data from rdbms to hdfs and exporting the data from hdfs to rdbms 

sqoop  ==>sql +  hadoop 

in scoop default number of mappers are 4 
no of reducers are zero 
why n reducers in sqoop 
  -->for aggregarion operation we require sqoop and in sqoop we require only select quiries for that mappers are sufficient 

diff b/w warehouse and target-dir  in sqoop

if wea are importing all the tabe in a particular database we require warehouse-dir 
if we are importing the data from particular table we require target-dir 

##If the table is not having a pk we have to choose numberof mappers = 1 because if a table having 1000 recordse tha table not having pk 
then 1000 records have to split t0 4 parts and in each part 250 records in map1 1-250 and in map2 251-500 like that 
but if the table not haing pk first 1-250 records are loaded into the mapper1 and mapper2 can't load because table not having pk 
we can overcome this problem by using --num-mappers 1 or -m 1 
the second option is we can also use --split-by columnname 

1.In import operation when we are using query tag the query tag must and should ends with $CONDITIONS . 
2.In eval operation when we are using query tag the query tag shouldn't end with $CONDITIONS  .


when running any sqoop job unfortunately job failed in a table we have 200 records out of 200 onlhy 50 are loaded into the hdfs now again run the hob then first 50 records will not reload again it will be taken care by metastoredb derby 

options-file must and should have only 400 permissions only 

vi file 
--connect
jdbc:mysql://localhost:3306
--username 
root 
--password 
hadoop 

:wq

1)list all the databses in rbdms mysql using sqoop

sqoop list-databases --options-file file 

2)list all tables in rdbms mysql kalyandatabse 

sqoop list-tables --connect "jdbc:mysql://localhost:3306/kalyan" --username root --password hadoop 

3)eval operation 

sqoop eval --options-file file --query 'show databases' 

4)The permissions for options-file is 400 only 

5)import all tables and put it in warehouse directory 

sqoop import --options-file file --warehouse-dir /sqoop/import-all-data/kalyan.db --exclude-tables orders,orders_items 

6)import a particular table sqoop.pet and put in hdfs with number of mappers 1 and saved as textfile

sqoop import --options-file file --table pet --target-dir /sqoop/import-data/sqoop.db/pet_m --as-textfile -m 1 

7)import a particular a table sqoop.pet with split-by option and saved as sequencefile target-dir /sqoop/inport-data/sqoop.db/pet 

sqoop import --options-file --table pet --target-dir /sqoop/import-data/sqoop.db/pet_split --as-sequencefile --split-by name 
   here split happens based on name column and no of mappers are 4 

8)case:In sqoop database there is a table called student having 36 records and having pk 
  a)using boundary query load only first 10 records and save as textfile fields lines terminated by 
  b)using boundary qury loas another 10 records append to it and number of mappers 1 fields lines terminated saved as textfile 
  c)using spli-by import id import the data here no of mappers4 lines fields terminated by save as textfile 

a)sqoop import --options-file file --table student --target-dir /sqoop/boundary-query/sqoop.db/student --boundary-query 'select 1,10 from student limit 1 ' --as-textfile --fields-terminated-by '\t' --lines-terminated-by '\n' -m 1 

b)sqoop import --options-file file --table student--target-dir /sqoop/boundary-query/sqoop.db/student/ --append --boundary-query 'select 11,20 from student limit 1' --as-textfile --fields-terminated-by '\t' --lines-terminated-by '\n' -m 1

c)sqoop import --options-file file --table student --target-dir /sqoop/boundary-query/sqoop.db/student --append --split-by id --lines-terminated-by '\n' --fields-terminated-by '\t' --as-textfile 

9)query opeartion 
 when doing query operation don't use table option because the o/p coming from query only we are loading into hdfs for that we require target-dir if table is having pk then no need to use no of mapers 1 here in example use --split-by name --num-mappers 2

sqoop import --options-file file --query 'select * from student where $CONDITIONS' --target-dir /sqoop/query/sqoop.db/student --split-by name --num-mappers 2 --fields-terminated-by '\t' --lines-terminated-by '\n' --as-textfile 

10)If a table student having 36 records load only 10 records using where condition and columns name,id 
   laod another 10 records using query operation using append remove duplicates (use incremental append 

sqoop import --options-file file --table student --target-dir /sqoop/data/sqoop.db/student --where "id <=10" --columns name,id --fields-terminated-by '\t' --lines-terminated-by '\n' --as-textfile 

sqoop import --options-file file --target-dir /sqoop/data/sqoop.db/student --query 'select * from student where id<=20 and $CONDITIONS' --as-textfile --fields-terminated-by '\t' --lines-terminated-by '\n' --append --incremental append --check-column id --last-value 10 

  when importing the data from rdbms to hdfs if a table having 1000 records we loaded some of them in to hdfs and second day we have to load remaining instead of seeing hdfs we have to do? The last value will be maintaind by metastore derby there is a concept called stat it wil maintain last value 


  





















